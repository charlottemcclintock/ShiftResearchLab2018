mf.df <-  arrange(mf.df, tf-idf)
mf.df <-  arrange(mf.df, -tf-idf)
View(mf.df)
mf.df <-  arrange(mf.df, -tf_idf)
mf.df <-  arrange(mf.df, -tf_idf)
mf.df[manifesto=="Democratic Party_1960"]
mf.df[, manifesto=="Democratic Party_1960"]
filter(mf.df, manifesto=="Democratic Party_1960")
mf.df <-  arrange(mf.df, -tf_idf)
filter(mf.df, manifesto=="Democratic Party_1960")
filter(mf.df, manifesto=="Republican Party_1960")
filter(mf.df, manifesto=="Democratic Party_2012")
filter(mf.df, manifesto=="Republican Party_2012")
View(manifesto.words)
mf.nrc <- manifesto.words %>%
inner_join(get_sentiments("nrc")) %>%
count(manifesto, sentiment)
View(mf.nrc)
manifesto.words <- unnest_tokens(manifestos, output = word, input = text) %>%
unite(manifesto, partyname, year, sep = "_", remove = FALSE)
word.count <- manifesto.words %>%
group_by(manifesto) %>%
count() %>%
summarize(totalwords = sum(n))
mf.df <- inner_join(manifesto.words, word.count, by = "manifesto")
data("stop_words")
mf.freq <- mf.df %>%
anti_join(stop_words)
mf.count <- mf.freq %>%
group_by(manifesto, word) %>%
count() %>%
summarize(n = sum(n))
mf.freq <- mf.count %>%
bind_tf_idf(word, manifesto, n)
mf.df <-  arrange(mf.df, -tf_idf)
mf.freq <-  arrange(mf.freq, -tf_idf)
filter(mf.freq, manifesto=="Democratic Party_1960")
filter(mf.freq, manifesto=="Republican Party_1960")
filter(mf.freq, manifesto=="Democratic Party_2012")
filter(mf.freq, manifesto=="Republican Party_2012")
mf.nrc <- manifesto.words %>%
inner_join(get_sentiments("nrc")) %>%
count(manifesto, sentiment)
mf.nrc <- mf.df %>%
inner_join(get_sentiments("nrc")) %>%
count(manifesto, sentiment)
View(mf.df)
View(mf.count)
View(word.count)
mf.nrc <- mf.df %>%
inner_join(get_sentiments("nrc")) %>%
count(manifesto, sentiment, wt=1/totalwords) %>%
inner_join(word.count, by = "manifesto")
View(manifestos)
mf.sent <- inner_join(mf.nrc, manifestos, by = "manifesto")
manifestos <- unite(manifestos, manifesto, partyname, year, sep = "_", remove = FALSE)
mf.sent <- inner_join(mf.nrc, manifestos, by = "manifesto")
View(mf.sent)
g <- ggplot(mf.sent, aes(x = proportion, y = presvote, colour=sentiment)) +
geom_point() +
geom_smooth(method="lm", se = FALSE) +
labs(x = "Proportion", y =  "Presidential Vote Share") +
facet_wrap(~sentiment, ncol = 2, scales = "free") +
coord_flip()
g
g <- ggplot(mf.sent, aes(x = n, y = presvote, colour=sentiment)) +
geom_point() +
geom_smooth(method="lm", se = FALSE) +
labs(x = "Proportion", y =  "Presidential Vote Share") +
facet_wrap(~sentiment, ncol = 2, scales = "free") +
coord_flip()
g
g <- ggplot(mf.sent, aes(x = n, y = presvote, colour=sentiment)) +
geom_point() +
geom_smooth(method="lm", se = FALSE) +
labs(x = "Proportion", y =  "Presidential Vote Share") +
facet_wrap(~sentiment, ncol = 2, scales = "free") +
coord_flip()
g
g <- ggplot(mf.sent, aes(x = n, y = presvote, colour=sentiment)) +
geom_point() +
geom_smooth(method="lm", se = FALSE) +
labs(x = "Proportion", y =  "Presidential Vote Share") +
facet_wrap(~sentiment, ncol = 2, scales = "free") +
coord_flip()
g
library(manifestoR)
mp_setapikey(key = API)
data <- mp_maindataset()
data <- filter(data, countryname == "United States")
data <- mutate(data, year = lubridate::year(edate))
data <- filter(data, year >=1960)
mpc <- mp_corpus(data, codefilter = NULL)
text <- sapply(mpc, FUN=function(x){
paste(x$content$text, collapse=" ")
})
manifestos <- data.frame(text = text,
year = c(1960, 1960, 1964, 1964, 1968,
1968, 1972, 1972, 1976, 1976,
1980, 1980, 1984, 1984, 1988,
1988, 1992, 1992, 1996, 1996,
2000, 2000, 2004, 2004, 2008,
2008, 2012, 2012),
partyname = rep(c("Democratic Party",
"Republican Party"), 14),
stringsAsFactors = FALSE)
rownames(manifestos) <- NULL
data <- dplyr::select(data, partyname, year, presvote)
manifestos <- full_join(manifestos, data)
manifestos$presvote[manifestos$year==2008 & manifestos$partyname == "Democratic Party"] <- 52.9
manifestos$presvote[manifestos$year==2008 & manifestos$partyname == "Republican Party"] <- 45.7
manifesto.words <- unnest_tokens(manifestos, output = word, input = text) %>%
unite(manifesto, partyname, year, sep = "_", remove = FALSE)
word.count <- manifesto.words %>%
group_by(manifesto) %>%
count() %>%
summarize(totalwords = sum(n))
mf.df <- inner_join(manifesto.words, word.count, by = "manifesto")
knitr::opts_chunk$set(echo = TRUE)
library(stargazer)
quantile(happy$medianage, .1, .3, .65, .91286)
knitr::opts_chunk$set(echo = TRUE)
library(stargazer)
library(knitr)
library(tidyverse)
library(tidytext)
library(margins)
oil.religion <- read_csv("oilreligion.csv")
happy <- read_csv("happy.csv", skip = 2) # header text
dim(oil.religion) # 253 rows, 3 columns
dim(happy) # 156 rows, 11 columns
happy <- select(happy, `Country`, `Happiness score`)
happy <- rename(happy, happy.score=`Happiness score`, country=Country)
oil.religion <- rename(oil.religion, religion=`I_RELIGION`, country=icountry)
oil.religion <- mutate(oil.religion, religion = as.factor(religion),
oilstate = as.factor(oilstate))
oil.religion <- mutate(oil.religion, religion=fct_recode(religion, NULL = "-999"),
oilstate=fct_recode(oilstate,
"oil-exporting"="1",
"non-oil-exporting"="0",
NULL = "-999"))
happy <- arrange(happy, happy.score)
kable(head(happy, 10), col.names = c("Country", "Happiness Score"))
happy <- arrange(happy, -happy.score)
kable(head(happy, 10), col.names = c("Country", "Happiness Score"))
intersect(names(happy), names(oil.religion))
unique.happy <- unique(select(happy, country))
nrow(unique.happy) # 156 rows
nrow(happy) # 156 rows
unique.oilrelig <- unique(select(oil.religion, country))
nrow(unique.oilrelig) # 253 rows
nrow(oil.religion) #253 rows
# countries in happy but not oil.religion
check1 <- anti_join(happy, oil.religion, by=c("country"))
# countries in oil.religion but not happy
check2 <- anti_join(oil.religion, happy, by=c("country"))
kable(check1)
# kable(check2) commented out because it takes up a ton of space in the markdown
oil.religion <- oil.religion %>%
mutate(country = fct_recode(country,
"United States" = "USA",
"United Kingdom" = "UK",
"United Arab Emirates" = "UAE",
"Taiwan Province of China" = "Taiwan",
"Trinidad & Tobago" = "Trinidad and Tobago",
"Hong Kong SAR, China" = "Hong Kong",
"Palestinian Territories" = "Palestine (Israeli Occupied Territories)",
"Ivory Coast" = "Cote d'Ivoire",
"Congo (Brazzaville)" = "Congo, Republic of the",
"Myanmar" = "Burma (Myanmar)",
"Congo (Kinshasa)" = "Congo, Democratic Republic of the"))
# check to see I didn't make any mistakes
check1 <- anti_join(happy, oil.religion, by=c("country"))
check2 <- anti_join(oil.religion, happy, by=c("country"))
kable(check1) # No match for South Sudan from the happy data in the oil.religion data
# probably because it's a new country??
# kable(check2)
happy <- inner_join(happy, oil.religion, by="country")
un <- read_csv("unindicators.csv")
un <- gather(un, Afghanistan:Zimbabwe, key="country", value="value")
un <- spread(un, var, value)
un <- mutate(un,
region=fct_recode(region,
"Asia/Oceania" = "Australia/New Zealand/Oceania",
"Asia/Oceania" = "Asia",
"Americas" = "Latin America/Caribbean",
"Americas" = "USA/Canada"))
# ID NAMES CHECK
intersect(names(happy), names(un)) # only shared variable name is country
# UNIQUE ID CHECK
unique.happy <- unique(select(happy, country))
nrow(unique.happy) # 156 rows
nrow(happy) # 156 rows
unique.un <- unique(select(un, country))
nrow(unique.un) # 253 rows
nrow(un) # 253 rows
# countries in happy but not oil.religion
check1 <- anti_join(happy, un, by=c("country"))
# countries in oil.religion but not happy
check2 <- anti_join(un, happy, by=c("country"))
kable(check1)
# kable(check2) # commented out because it takes up a lot of space in the markdown
un <- un %>%
mutate(country = fct_recode(country,
"Taiwan Province of China" = "Taiwan",
"Trinidad & Tobago" = "Trinidad and Tobago",
"Hong Kong SAR, China" = "Hong Kong",
"South Korea" = "Korea, South",
"Palestinian Territories" = "Palestine (Israeli Occupied Territories)",
"Ivory Coast" = "Cote d'Ivoire",
"Congo (Brazzaville)" = "Congo, Republic of the",
"Myanmar" = "Burma (Myanmar)",
"Congo (Kinshasa)" = "Congo, Democratic Republic of the"))
# check to see I didn't make any mistakes
check1 <- anti_join(happy, un, by=c("country"))
check2 <- anti_join(un, happy, by=c("country"))
happy <- inner_join(happy, un, by="country")
str(happy)
happy <- happy %>%
mutate(
gdpcap = as.numeric(gdpcap),
GINI = as.numeric(GINI),
literacy = as.numeric(literacy),
medianage = as.numeric(medianage),
population = as.numeric(population),
population = population/1000000,
povertyindex = as.numeric(povertyindex),
povertyindex = 100*povertyindex,
happy.bin = ifelse(happy.score>4.5, TRUE, FALSE))
quantile(happy$medianage, probs=c(.1, .3, .65, .91286), na.rm=TRUE)
levels(happy$oilstate)
t.test(happy.score ~ oilstate, data=happy)
levels(happy$oilstate)
install.packages("tidyverse")
install.packages("forcats")
install.packages("dplyr")
install.packages("tidyr")
install.packages("lubridate")
install.packages("stringr")
install.packages("chorddiag")
install.packages("chorddiag")
install.packages("htmlwidgets")
install.packages("chorddiag")
devtools::install_github("rstudio/r2d3")
install.packages("devtools")
# set up: wd, retrieve encrypted data
rm(list=ls())
getwd()
# if need be setwd("~/../../")
setwd("R/ShiftResearchLab2018/ed-data")
library(dplyr)
library(forcats)
library(tidyverse)
library(readxl)
library(tidyr)
hs <- read_excel("hsgrad.xlsx")
View(hs)
?drop
hs <- hs[-1,]
View(hs)
View(hs)
hs2 <- select(hs, `County.Name`, `Organization.Code` )
names(hs)
hs <- select(hs, `County Name`, `Organization Code`, `Organization Name`,
`School Code`, `School Name`, `\"Class Of…\" Anticpated Year of Graduation Cohort`,
`All Students Graduation Rate`)
View(hs)
names(hs) <- c("county", "distcode", "district", "schoolcode", "school", "cohort", "gradrate")
# read in the high school graduation data
hs <- read_excel("hsgrad.xlsx")
View(hs)
## C. McClintock
## Shift Research Lab
## Summer 2018 ## Updated: May 22, 2018
## Cleaning Script: ACS 5-year Commute Data
# ..................................................................................................
# set up: wd, retrieve encrypted data
rm(list=ls())
getwd()
# if need be setwd("~/../../")
setwd("R/ShiftResearchLab2018/commute")
# set up: libraries
library(dplyr)
library(forcats)
library(tidyverse)
library(readxl)
library(tidyr)
# ..................................................................................................
# AVERAGE COMMUTE TIME
# read in the average commute time data (ACS 2016)
avg <- read_csv("avg.commute.csv")
# select just the first row
avg <- avg[1,]
# move columns to rows
avg <- gather(avg, tract, avgcommute, 2:ncol(avg))
avg <- select(avg, -X1)
# ..................................................................................................
# TRAVEL TIME DISTRIBUTIONS
# read in commute distributions (ACS 2016)
travel <- read_csv("traveltime.csv")
travel <- travel[1:10,] # select rows with values
# reshape the data to be useful
travel <- gather(travel, tract, n, 2:ncol(travel))
travel <- spread(travel, X1, n)
# rename the variables for ease of use
names(travel)
names(travel) <- c("tract", "10to19", "20to29", "30to39", "40to59",
"60to89", "90plus", "total_outside", "less10",
"workedhome", "total")
# reorder the columns
travel <- select(travel, tract, less10, everything())
# merge the data
commute <- left_join(travel, avg, by="tract")
# read in the tract information
info <- read_csv("acs.meta.csv")
info <- select(info, Geo_FIPS, Geo_NAME, Geo_TRACT)
names(info) <- c("geo_id", "tract", "tract_num")
# merge with commute info
commute <- left_join(info, commute, by="tract")
# ..................................................................................................
# MATCHING TRACTS TO NEIGHBORHOODS
# read in matching tracts data
match <- read_csv("matchingtracts.csv")
match <- rename(match, "geo_id"="geoid10")
# merge with commute data
nbhd <- full_join(match, commute, by="geo_id")
# break up the big tract variable
nbhd <- separate(nbhd, tract, c("tract", "county", "state"), sep = ",")
nbhd <- select(nbhd, -c(tract, state))
n_distinct(nbhd$nhid) # 265
n_distinct(nbhd$nhname) # 264 # why is this one less?
# n occurences of each neighborhood name
nhname.freq <- nbhd %>%
count(nhname) # 576 NAs, tracts that don't match a neighborhood
# n occurences of each neighborhood id
nhid.freq <- nbhd %>%
count(nhid) # 576 NAs, tracts that don't match a neighborhood
arb <- filter(nbhd, is.na(nhname)&!is.na(nhid)) # no values so nhname is not blank
str(nbhd) # check the class of the variables
nbhd$avgcommute <- as.numeric(nbhd$avgcommute) # coerce avg commute to numeric class
nbhd$nhid <- as.numeric(nbhd$nhid)
# what's the average denver metro commute time?
mean(nbhd$avgcommute, na.rm=TRUE) # 26.57
median(nbhd$avgcommute, na.rm = TRUE) # 26
# compute neighborhood average commute time
nbhd.avg <- aggregate(nbhd$avgcommute,by=list(name=nbhd$nhname, nhid=nbhd$nhid), data=nbhd, FUN=mean)
nbhd.avg <- rename(nbhd.avg, "avgcommute"="x")
nrow(nbhd.avg) # 264
# 7 NA values, why?
na.check <- filter(nbhd.avg, is.na(avgcommute))
# check with NAs in avg commute removed
nbhd2 <- filter(nbhd, !is.na(nbhd$avgcommute)) # remove observations with NA in avg commute
nbhd.avg2 <- aggregate(nbhd2$avgcommute,by=list(name=nbhd2$nhname, nhid=nbhd2$nhid), data=nbhd2, FUN=mean)
nbhd.avg2 <- rename(nbhd.avg2, "avgcommute"="x")
nrow(nbhd.avg2) # 262 # 2 missing
# check which two are missing
arb <- anti_join(nbhd.avg, nbhd.avg2, by="name")
# Federal Center # Rocky Mountain Arsenal
# one tract each with a missing avg commute and missing travel time, will be excluded
# rename for ease of use
avgcmt <- nbhd.avg2
# ..................................................................................................
# ADD SOME POTENTIALLY USEFUL MEASURES
mean(avgcmt$avgcommute) # 26.47
avgcmt <- mutate(avgcmt, # difference between nbhd and avg
meandelta=avgcommute-mean(avgcmt$avgcommute))
# ..................................................................................................
# write the object as a csv for later use
write.csv(nbhd, "clean.nbhdcommutes.csv")
write.csv(nbhd.avg2, "clean.avgcommute.csv")
# ..................................................................................................
# TO DO LIST:
# figure out why there are 265 nhids and 264 nhnames
getwd()
# if need be
setwd("~/../../")
getwd()
# if need be setwd("~/../../")
setwd("/Users/charmed33/R/ShiftResearchLab2018/commute")
# if need be setwd("~/../../")
setwd("R/ShiftResearchLab2018/commute")
## C. McClintock
## Shift Research Lab
## Summer 2018 ## Updated: May 22, 2018
## Cleaning Script: ACS 5-year Commute Data
# ..................................................................................................
# set up: wd, retrieve encrypted data
rm(list=ls())
getwd()
# if need be setwd("~/../../")
setwd("R/ShiftResearchLab2018/commute")
# set up: libraries
library(dplyr)
library(forcats)
library(tidyverse)
library(readxl)
library(tidyr)
# ..................................................................................................
# AVERAGE COMMUTE TIME
# read in the average commute time data (ACS 2016)
avg <- read_csv("avg.commute.csv")
# select just the first row
avg <- avg[1,]
# move columns to rows
avg <- gather(avg, tract, avgcommute, 2:ncol(avg))
avg <- select(avg, -X1)
# ..................................................................................................
# TRAVEL TIME DISTRIBUTIONS
# read in commute distributions (ACS 2016)
travel <- read_csv("traveltime.csv")
travel <- travel[1:10,] # select rows with values
# reshape the data to be useful
travel <- gather(travel, tract, n, 2:ncol(travel))
travel <- spread(travel, X1, n)
# rename the variables for ease of use
names(travel)
names(travel) <- c("tract", "10to19", "20to29", "30to39", "40to59",
"60to89", "90plus", "total_outside", "less10",
"workedhome", "total")
# reorder the columns
travel <- select(travel, tract, less10, everything())
# merge the data
commute <- left_join(travel, avg, by="tract")
# read in the tract information
info <- read_csv("acs.meta.csv")
info <- select(info, Geo_FIPS, Geo_NAME, Geo_TRACT)
names(info) <- c("geo_id", "tract", "tract_num")
# merge with commute info
commute <- left_join(info, commute, by="tract")
# ..................................................................................................
# MATCHING TRACTS TO NEIGHBORHOODS
# read in matching tracts data
match <- read_csv("matchingtracts.csv")
match <- rename(match, "geo_id"="geoid10")
# merge with commute data
nbhd <- full_join(match, commute, by="geo_id")
# break up the big tract variable
nbhd <- separate(nbhd, tract, c("tract", "county", "state"), sep = ",")
nbhd <- select(nbhd, -c(tract, state))
n_distinct(nbhd$nhid) # 265
n_distinct(nbhd$nhname) # 264 # why is this one less?
# n occurences of each neighborhood name
nhname.freq <- nbhd %>%
count(nhname) # 576 NAs, tracts that don't match a neighborhood
# n occurences of each neighborhood id
nhid.freq <- nbhd %>%
count(nhid) # 576 NAs, tracts that don't match a neighborhood
arb <- filter(nbhd, is.na(nhname)&!is.na(nhid)) # no values so nhname is not blank
str(nbhd) # check the class of the variables
nbhd$avgcommute <- as.numeric(nbhd$avgcommute) # coerce avg commute to numeric class
nbhd$nhid <- as.numeric(nbhd$nhid)
# what's the average denver metro commute time?
mean(nbhd$avgcommute, na.rm=TRUE) # 26.57
median(nbhd$avgcommute, na.rm = TRUE) # 26
# compute neighborhood average commute time
nbhd.avg <- aggregate(nbhd$avgcommute,by=list(name=nbhd$nhname, nhid=nbhd$nhid), data=nbhd, FUN=mean)
nbhd.avg <- rename(nbhd.avg, "avgcommute"="x")
nrow(nbhd.avg) # 264
# 7 NA values, why?
na.check <- filter(nbhd.avg, is.na(avgcommute))
# check with NAs in avg commute removed
nbhd2 <- filter(nbhd, !is.na(nbhd$avgcommute)) # remove observations with NA in avg commute
nbhd.avg2 <- aggregate(nbhd2$avgcommute,by=list(name=nbhd2$nhname, nhid=nbhd2$nhid), data=nbhd2, FUN=mean)
nbhd.avg2 <- rename(nbhd.avg2, "avgcommute"="x")
nrow(nbhd.avg2) # 262 # 2 missing
# check which two are missing
arb <- anti_join(nbhd.avg, nbhd.avg2, by="name")
# Federal Center # Rocky Mountain Arsenal
# one tract each with a missing avg commute and missing travel time, will be excluded
# rename for ease of use
avgcmt <- nbhd.avg2
# ..................................................................................................
# ADD SOME POTENTIALLY USEFUL MEASURES
mean(avgcmt$avgcommute) # 26.47
avgcmt <- mutate(avgcmt, # difference between nbhd and avg
meandelta=avgcommute-mean(avgcmt$avgcommute))
# ..................................................................................................
# write the object as a csv for later use
write.csv(nbhd, "clean.nbhdcommutes.csv")
write.csv(nbhd.avg2, "clean.avgcommute.csv")
# ..................................................................................................
# TO DO LIST:
# figure out why there are 265 nhids and 264 nhnames
## C. McClintock
## Shift Research Lab
## Summer 2018 ## Updated: May 22, 2018
## Cleaning Script: CDE Education Data
# ..................................................................................................
# set up: wd, retrieve encrypted data
rm(list=ls())
getwd()
# if need be setwd("~/../../")
setwd("R/ShiftResearchLab2018/ed-data")
# set up: libraries
library(dplyr)
library(forcats)
library(tidyverse)
library(readxl)
library(tidyr)
# ..................................................................................................
# HIGH SCHOOL GRADUATION RATES
# read in the high school graduation data
hs <- read_excel("hsgrad.xlsx")
hs <- hs[-1,] # delete the first row of Excel formulas
hs <- select(hs, `County Name`, `Organization Code`, `Organization Name`,
`School Code`, `School Name`, `\"Class Of…\" Anticpated Year of Graduation Cohort`,
`All Students Graduation Rate`)
# better names
names(hs)
names(hs) <- c("county", "distcode", "district", "schoolcode", "school", "cohort", "gradrate")
# ..................................................................................................
# write the useful objects as CSVs
